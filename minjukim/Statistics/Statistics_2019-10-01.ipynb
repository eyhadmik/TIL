{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Advanced (Continue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>burnout</th>\n",
       "      <th>loc</th>\n",
       "      <th>cope</th>\n",
       "      <th>teaching</th>\n",
       "      <th>research</th>\n",
       "      <th>pastoral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>Not Burnt Out</td>\n",
       "      <td>8.235294</td>\n",
       "      <td>22.137405</td>\n",
       "      <td>50.909091</td>\n",
       "      <td>56.25</td>\n",
       "      <td>61.111111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           burnout       loc       cope   teaching  research   pastoral\n",
       "112  Not Burnt Out  8.235294  22.137405  50.909091     56.25  61.111111"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "burnout = pd.read_csv('burnout.csv')\n",
    "\n",
    "burnout.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Dummy Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>burnout</th>\n",
       "      <th>loc</th>\n",
       "      <th>cope</th>\n",
       "      <th>teaching</th>\n",
       "      <th>research</th>\n",
       "      <th>pastoral</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Not Burnt Out</td>\n",
       "      <td>7.647059</td>\n",
       "      <td>9.160305</td>\n",
       "      <td>32.727273</td>\n",
       "      <td>87.500000</td>\n",
       "      <td>31.481481</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not Burnt Out</td>\n",
       "      <td>6.470588</td>\n",
       "      <td>12.977099</td>\n",
       "      <td>52.727273</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>68.518519</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not Burnt Out</td>\n",
       "      <td>8.823529</td>\n",
       "      <td>9.160305</td>\n",
       "      <td>49.090909</td>\n",
       "      <td>60.416667</td>\n",
       "      <td>53.703704</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Not Burnt Out</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>9.160305</td>\n",
       "      <td>52.727273</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Not Burnt Out</td>\n",
       "      <td>6.470588</td>\n",
       "      <td>19.083969</td>\n",
       "      <td>43.636364</td>\n",
       "      <td>79.166667</td>\n",
       "      <td>40.740741</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Not Burnt Out</td>\n",
       "      <td>7.058824</td>\n",
       "      <td>16.030534</td>\n",
       "      <td>38.181818</td>\n",
       "      <td>52.083333</td>\n",
       "      <td>48.148148</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Not Burnt Out</td>\n",
       "      <td>8.235294</td>\n",
       "      <td>16.793893</td>\n",
       "      <td>16.363636</td>\n",
       "      <td>58.333333</td>\n",
       "      <td>37.037037</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Not Burnt Out</td>\n",
       "      <td>13.529412</td>\n",
       "      <td>7.633588</td>\n",
       "      <td>43.636364</td>\n",
       "      <td>70.833333</td>\n",
       "      <td>35.185185</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Not Burnt Out</td>\n",
       "      <td>8.823529</td>\n",
       "      <td>9.160305</td>\n",
       "      <td>41.818182</td>\n",
       "      <td>81.250000</td>\n",
       "      <td>46.296296</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Not Burnt Out</td>\n",
       "      <td>7.058824</td>\n",
       "      <td>13.740458</td>\n",
       "      <td>56.363636</td>\n",
       "      <td>77.083333</td>\n",
       "      <td>62.962963</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Not Burnt Out</td>\n",
       "      <td>12.352941</td>\n",
       "      <td>12.213740</td>\n",
       "      <td>58.181818</td>\n",
       "      <td>81.250000</td>\n",
       "      <td>48.148148</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Not Burnt Out</td>\n",
       "      <td>12.352941</td>\n",
       "      <td>14.503817</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>39.583333</td>\n",
       "      <td>53.703704</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Not Burnt Out</td>\n",
       "      <td>7.058824</td>\n",
       "      <td>35.114504</td>\n",
       "      <td>63.636364</td>\n",
       "      <td>64.583333</td>\n",
       "      <td>61.111111</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Not Burnt Out</td>\n",
       "      <td>9.411765</td>\n",
       "      <td>9.923664</td>\n",
       "      <td>36.363636</td>\n",
       "      <td>41.666667</td>\n",
       "      <td>37.037037</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Not Burnt Out</td>\n",
       "      <td>11.764706</td>\n",
       "      <td>19.847328</td>\n",
       "      <td>67.272727</td>\n",
       "      <td>60.416667</td>\n",
       "      <td>70.370370</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Not Burnt Out</td>\n",
       "      <td>14.117647</td>\n",
       "      <td>9.923664</td>\n",
       "      <td>49.090909</td>\n",
       "      <td>79.166667</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Not Burnt Out</td>\n",
       "      <td>24.705882</td>\n",
       "      <td>6.870229</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>95.833333</td>\n",
       "      <td>24.074074</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Not Burnt Out</td>\n",
       "      <td>11.176471</td>\n",
       "      <td>9.160305</td>\n",
       "      <td>45.454545</td>\n",
       "      <td>79.166667</td>\n",
       "      <td>48.148148</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Not Burnt Out</td>\n",
       "      <td>8.235294</td>\n",
       "      <td>9.923664</td>\n",
       "      <td>32.727273</td>\n",
       "      <td>79.166667</td>\n",
       "      <td>62.962963</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Not Burnt Out</td>\n",
       "      <td>17.647059</td>\n",
       "      <td>11.450382</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>39.583333</td>\n",
       "      <td>77.777778</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Not Burnt Out</td>\n",
       "      <td>6.470588</td>\n",
       "      <td>12.213740</td>\n",
       "      <td>50.909091</td>\n",
       "      <td>70.833333</td>\n",
       "      <td>48.148148</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Not Burnt Out</td>\n",
       "      <td>16.470588</td>\n",
       "      <td>7.633588</td>\n",
       "      <td>58.181818</td>\n",
       "      <td>81.250000</td>\n",
       "      <td>44.444444</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Not Burnt Out</td>\n",
       "      <td>19.411765</td>\n",
       "      <td>11.450382</td>\n",
       "      <td>58.181818</td>\n",
       "      <td>77.083333</td>\n",
       "      <td>48.148148</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Not Burnt Out</td>\n",
       "      <td>9.411765</td>\n",
       "      <td>19.083969</td>\n",
       "      <td>56.363636</td>\n",
       "      <td>72.916667</td>\n",
       "      <td>61.111111</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Not Burnt Out</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>24.427481</td>\n",
       "      <td>43.636364</td>\n",
       "      <td>39.583333</td>\n",
       "      <td>40.740741</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Not Burnt Out</td>\n",
       "      <td>10.588235</td>\n",
       "      <td>15.267176</td>\n",
       "      <td>58.181818</td>\n",
       "      <td>77.083333</td>\n",
       "      <td>53.703704</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Not Burnt Out</td>\n",
       "      <td>10.588235</td>\n",
       "      <td>10.687023</td>\n",
       "      <td>41.818182</td>\n",
       "      <td>56.250000</td>\n",
       "      <td>42.592593</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Not Burnt Out</td>\n",
       "      <td>6.470588</td>\n",
       "      <td>32.824427</td>\n",
       "      <td>50.909091</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>51.851852</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Not Burnt Out</td>\n",
       "      <td>12.352941</td>\n",
       "      <td>10.687023</td>\n",
       "      <td>49.090909</td>\n",
       "      <td>56.250000</td>\n",
       "      <td>18.518519</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Not Burnt Out</td>\n",
       "      <td>6.470588</td>\n",
       "      <td>12.977099</td>\n",
       "      <td>25.454545</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>57.407407</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>Burnt Out</td>\n",
       "      <td>24.117647</td>\n",
       "      <td>47.328244</td>\n",
       "      <td>61.818182</td>\n",
       "      <td>52.083333</td>\n",
       "      <td>68.518519</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>Burnt Out</td>\n",
       "      <td>22.352941</td>\n",
       "      <td>66.412214</td>\n",
       "      <td>76.363636</td>\n",
       "      <td>58.333333</td>\n",
       "      <td>75.925926</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>Burnt Out</td>\n",
       "      <td>34.117647</td>\n",
       "      <td>35.114504</td>\n",
       "      <td>49.090909</td>\n",
       "      <td>70.833333</td>\n",
       "      <td>51.851852</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>Burnt Out</td>\n",
       "      <td>22.941176</td>\n",
       "      <td>50.381679</td>\n",
       "      <td>69.090909</td>\n",
       "      <td>72.916667</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>Burnt Out</td>\n",
       "      <td>35.882353</td>\n",
       "      <td>36.641221</td>\n",
       "      <td>54.545455</td>\n",
       "      <td>64.583333</td>\n",
       "      <td>72.222222</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>Burnt Out</td>\n",
       "      <td>23.529412</td>\n",
       "      <td>41.221374</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>68.750000</td>\n",
       "      <td>57.407407</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>Burnt Out</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>38.167939</td>\n",
       "      <td>49.090909</td>\n",
       "      <td>47.916667</td>\n",
       "      <td>38.888889</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>Burnt Out</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>38.167939</td>\n",
       "      <td>58.181818</td>\n",
       "      <td>58.333333</td>\n",
       "      <td>61.111111</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>Burnt Out</td>\n",
       "      <td>42.941176</td>\n",
       "      <td>39.694656</td>\n",
       "      <td>94.545455</td>\n",
       "      <td>60.416667</td>\n",
       "      <td>87.037037</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>Burnt Out</td>\n",
       "      <td>27.058824</td>\n",
       "      <td>38.931298</td>\n",
       "      <td>52.727273</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>44.444444</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>Burnt Out</td>\n",
       "      <td>24.117647</td>\n",
       "      <td>57.251908</td>\n",
       "      <td>87.272727</td>\n",
       "      <td>41.666667</td>\n",
       "      <td>62.962963</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>Burnt Out</td>\n",
       "      <td>24.705882</td>\n",
       "      <td>38.931298</td>\n",
       "      <td>63.636364</td>\n",
       "      <td>72.916667</td>\n",
       "      <td>72.222222</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>Burnt Out</td>\n",
       "      <td>43.529412</td>\n",
       "      <td>54.961832</td>\n",
       "      <td>72.727273</td>\n",
       "      <td>31.250000</td>\n",
       "      <td>68.518519</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>Burnt Out</td>\n",
       "      <td>31.764706</td>\n",
       "      <td>45.038168</td>\n",
       "      <td>50.909091</td>\n",
       "      <td>45.833333</td>\n",
       "      <td>70.370370</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>Burnt Out</td>\n",
       "      <td>45.294118</td>\n",
       "      <td>42.748092</td>\n",
       "      <td>67.272727</td>\n",
       "      <td>35.416667</td>\n",
       "      <td>59.259259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>Burnt Out</td>\n",
       "      <td>32.941176</td>\n",
       "      <td>56.488550</td>\n",
       "      <td>61.818182</td>\n",
       "      <td>45.833333</td>\n",
       "      <td>55.555556</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>Burnt Out</td>\n",
       "      <td>27.647059</td>\n",
       "      <td>70.992366</td>\n",
       "      <td>85.454545</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>64.814815</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>Burnt Out</td>\n",
       "      <td>27.058824</td>\n",
       "      <td>48.854962</td>\n",
       "      <td>61.818182</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>75.925926</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>Burnt Out</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>51.145038</td>\n",
       "      <td>78.181818</td>\n",
       "      <td>47.916667</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>Burnt Out</td>\n",
       "      <td>28.823529</td>\n",
       "      <td>55.725191</td>\n",
       "      <td>45.454545</td>\n",
       "      <td>52.083333</td>\n",
       "      <td>38.888889</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>Burnt Out</td>\n",
       "      <td>31.176471</td>\n",
       "      <td>92.366412</td>\n",
       "      <td>92.727273</td>\n",
       "      <td>45.833333</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>Burnt Out</td>\n",
       "      <td>31.176471</td>\n",
       "      <td>53.435115</td>\n",
       "      <td>61.818182</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>62.962963</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>Burnt Out</td>\n",
       "      <td>31.176471</td>\n",
       "      <td>83.206107</td>\n",
       "      <td>74.545455</td>\n",
       "      <td>35.416667</td>\n",
       "      <td>87.037037</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>Burnt Out</td>\n",
       "      <td>32.941176</td>\n",
       "      <td>67.938931</td>\n",
       "      <td>65.454545</td>\n",
       "      <td>41.666667</td>\n",
       "      <td>79.629630</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>Burnt Out</td>\n",
       "      <td>36.470588</td>\n",
       "      <td>54.198473</td>\n",
       "      <td>65.454545</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>72.222222</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>Burnt Out</td>\n",
       "      <td>33.529412</td>\n",
       "      <td>58.778626</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>64.583333</td>\n",
       "      <td>75.925926</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>Burnt Out</td>\n",
       "      <td>51.764706</td>\n",
       "      <td>54.961832</td>\n",
       "      <td>61.818182</td>\n",
       "      <td>56.250000</td>\n",
       "      <td>59.259259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>Burnt Out</td>\n",
       "      <td>35.294118</td>\n",
       "      <td>56.488550</td>\n",
       "      <td>85.454545</td>\n",
       "      <td>72.916667</td>\n",
       "      <td>94.444444</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>Burnt Out</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>58.015267</td>\n",
       "      <td>76.363636</td>\n",
       "      <td>64.583333</td>\n",
       "      <td>94.444444</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>Burnt Out</td>\n",
       "      <td>61.764706</td>\n",
       "      <td>81.679389</td>\n",
       "      <td>94.545455</td>\n",
       "      <td>81.250000</td>\n",
       "      <td>96.296296</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>467 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           burnout        loc       cope   teaching   research    pastoral  y\n",
       "0    Not Burnt Out   7.647059   9.160305  32.727273  87.500000   31.481481  0\n",
       "1    Not Burnt Out   6.470588  12.977099  52.727273  66.666667   68.518519  0\n",
       "2    Not Burnt Out   8.823529   9.160305  49.090909  60.416667   53.703704  0\n",
       "3    Not Burnt Out  20.000000   9.160305  52.727273  62.500000   50.000000  0\n",
       "4    Not Burnt Out   6.470588  19.083969  43.636364  79.166667   40.740741  0\n",
       "5    Not Burnt Out   7.058824  16.030534  38.181818  52.083333   48.148148  0\n",
       "6    Not Burnt Out   8.235294  16.793893  16.363636  58.333333   37.037037  0\n",
       "7    Not Burnt Out  13.529412   7.633588  43.636364  70.833333   35.185185  0\n",
       "8    Not Burnt Out   8.823529   9.160305  41.818182  81.250000   46.296296  0\n",
       "9    Not Burnt Out   7.058824  13.740458  56.363636  77.083333   62.962963  0\n",
       "10   Not Burnt Out  12.352941  12.213740  58.181818  81.250000   48.148148  0\n",
       "11   Not Burnt Out  12.352941  14.503817  40.000000  39.583333   53.703704  0\n",
       "12   Not Burnt Out   7.058824  35.114504  63.636364  64.583333   61.111111  0\n",
       "13   Not Burnt Out   9.411765   9.923664  36.363636  41.666667   37.037037  0\n",
       "14   Not Burnt Out  11.764706  19.847328  67.272727  60.416667   70.370370  0\n",
       "15   Not Burnt Out  14.117647   9.923664  49.090909  79.166667   33.333333  0\n",
       "16   Not Burnt Out  24.705882   6.870229  40.000000  95.833333   24.074074  0\n",
       "17   Not Burnt Out  11.176471   9.160305  45.454545  79.166667   48.148148  0\n",
       "18   Not Burnt Out   8.235294   9.923664  32.727273  79.166667   62.962963  0\n",
       "19   Not Burnt Out  17.647059  11.450382  60.000000  39.583333   77.777778  0\n",
       "20   Not Burnt Out   6.470588  12.213740  50.909091  70.833333   48.148148  0\n",
       "21   Not Burnt Out  16.470588   7.633588  58.181818  81.250000   44.444444  0\n",
       "22   Not Burnt Out  19.411765  11.450382  58.181818  77.083333   48.148148  0\n",
       "23   Not Burnt Out   9.411765  19.083969  56.363636  72.916667   61.111111  0\n",
       "24   Not Burnt Out  10.000000  24.427481  43.636364  39.583333   40.740741  0\n",
       "25   Not Burnt Out  10.588235  15.267176  58.181818  77.083333   53.703704  0\n",
       "26   Not Burnt Out  10.588235  10.687023  41.818182  56.250000   42.592593  0\n",
       "27   Not Burnt Out   6.470588  32.824427  50.909091  75.000000   51.851852  0\n",
       "28   Not Burnt Out  12.352941  10.687023  49.090909  56.250000   18.518519  0\n",
       "29   Not Burnt Out   6.470588  12.977099  25.454545  62.500000   57.407407  0\n",
       "..             ...        ...        ...        ...        ...         ... ..\n",
       "437      Burnt Out  24.117647  47.328244  61.818182  52.083333   68.518519  1\n",
       "438      Burnt Out  22.352941  66.412214  76.363636  58.333333   75.925926  1\n",
       "439      Burnt Out  34.117647  35.114504  49.090909  70.833333   51.851852  1\n",
       "440      Burnt Out  22.941176  50.381679  69.090909  72.916667   66.666667  1\n",
       "441      Burnt Out  35.882353  36.641221  54.545455  64.583333   72.222222  1\n",
       "442      Burnt Out  23.529412  41.221374  60.000000  68.750000   57.407407  1\n",
       "443      Burnt Out  30.000000  38.167939  49.090909  47.916667   38.888889  1\n",
       "444      Burnt Out  30.000000  38.167939  58.181818  58.333333   61.111111  1\n",
       "445      Burnt Out  42.941176  39.694656  94.545455  60.416667   87.037037  1\n",
       "446      Burnt Out  27.058824  38.931298  52.727273  33.333333   44.444444  1\n",
       "447      Burnt Out  24.117647  57.251908  87.272727  41.666667   62.962963  1\n",
       "448      Burnt Out  24.705882  38.931298  63.636364  72.916667   72.222222  1\n",
       "449      Burnt Out  43.529412  54.961832  72.727273  31.250000   68.518519  1\n",
       "450      Burnt Out  31.764706  45.038168  50.909091  45.833333   70.370370  1\n",
       "451      Burnt Out  45.294118  42.748092  67.272727  35.416667   59.259259  1\n",
       "452      Burnt Out  32.941176  56.488550  61.818182  45.833333   55.555556  1\n",
       "453      Burnt Out  27.647059  70.992366  85.454545  75.000000   64.814815  1\n",
       "454      Burnt Out  27.058824  48.854962  61.818182  33.333333   75.925926  1\n",
       "455      Burnt Out  30.000000  51.145038  78.181818  47.916667   66.666667  1\n",
       "456      Burnt Out  28.823529  55.725191  45.454545  52.083333   38.888889  1\n",
       "457      Burnt Out  31.176471  92.366412  92.727273  45.833333  100.000000  1\n",
       "458      Burnt Out  31.176471  53.435115  61.818182  62.500000   62.962963  1\n",
       "459      Burnt Out  31.176471  83.206107  74.545455  35.416667   87.037037  1\n",
       "460      Burnt Out  32.941176  67.938931  65.454545  41.666667   79.629630  1\n",
       "461      Burnt Out  36.470588  54.198473  65.454545  62.500000   72.222222  1\n",
       "462      Burnt Out  33.529412  58.778626  80.000000  64.583333   75.925926  1\n",
       "463      Burnt Out  51.764706  54.961832  61.818182  56.250000   59.259259  1\n",
       "464      Burnt Out  35.294118  56.488550  85.454545  72.916667   94.444444  1\n",
       "465      Burnt Out  70.000000  58.015267  76.363636  64.583333   94.444444  1\n",
       "466      Burnt Out  61.764706  81.679389  94.545455  81.250000   96.296296  1\n",
       "\n",
       "[467 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "burnout['y'] = burnout['burnout'].replace({'Burnt Out': 1, 'Not Burnt Out': 0})\n",
    "\n",
    "burnout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.formula.api import glm\n",
    "from statsmodels.genmod.families.family import Binomial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### glm: Generalized Linear Model \n",
    "- 선형모형을 일반화\n",
    "- family 에 넣는 값에 따라 달라진다\n",
    "- cf. olm 은 기본 선형모형만 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cope y(burnout될 확률)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = glm('y ~ cope', burnout, family=Binomial()).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Generalized Linear Model Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>y</td>        <th>  No. Observations:  </th>  <td>   467</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>GLM</td>       <th>  Df Residuals:      </th>  <td>   465</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model Family:</th>       <td>Binomial</td>     <th>  Df Model:          </th>  <td>     1</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Link Function:</th>        <td>logit</td>      <th>  Scale:             </th> <td>  1.0000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>IRLS</td>       <th>  Log-Likelihood:    </th> <td> -199.52</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>           <td>Tue, 01 Oct 2019</td> <th>  Deviance:          </th> <td>  399.03</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>               <td>10:34:12</td>     <th>  Pearson chi2:      </th>  <td>  575.</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Iterations:</th>         <td>5</td>        <th>  Covariance Type:   </th> <td>nonrobust</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>   -3.3839</td> <td>    0.283</td> <td>  -11.943</td> <td> 0.000</td> <td>   -3.939</td> <td>   -2.829</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cope</th>      <td>    0.0862</td> <td>    0.009</td> <td>    9.519</td> <td> 0.000</td> <td>    0.068</td> <td>    0.104</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                 Generalized Linear Model Regression Results                  \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   No. Observations:                  467\n",
       "Model:                            GLM   Df Residuals:                      465\n",
       "Model Family:                Binomial   Df Model:                            1\n",
       "Link Function:                  logit   Scale:                          1.0000\n",
       "Method:                          IRLS   Log-Likelihood:                -199.52\n",
       "Date:                Tue, 01 Oct 2019   Deviance:                       399.03\n",
       "Time:                        10:34:12   Pearson chi2:                     575.\n",
       "No. Iterations:                     5   Covariance Type:             nonrobust\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept     -3.3839      0.283    -11.943      0.000      -3.939      -2.829\n",
       "cope           0.0862      0.009      9.519      0.000       0.068       0.104\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "403.03285727663285"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.aic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2459.010247539404"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.bic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### teaching y(burnout될 확률)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = glm('y ~ teaching', burnout, family=Binomial()).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Generalized Linear Model Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>y</td>        <th>  No. Observations:  </th>  <td>   467</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>GLM</td>       <th>  Df Residuals:      </th>  <td>   465</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model Family:</th>       <td>Binomial</td>     <th>  Df Model:          </th>  <td>     1</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Link Function:</th>        <td>logit</td>      <th>  Scale:             </th> <td>  1.0000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>IRLS</td>       <th>  Log-Likelihood:    </th> <td> -247.30</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>           <td>Tue, 01 Oct 2019</td> <th>  Deviance:          </th> <td>  494.60</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>               <td>10:35:52</td>     <th>  Pearson chi2:      </th>  <td>  457.</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Iterations:</th>         <td>4</td>        <th>  Covariance Type:   </th> <td>nonrobust</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>   -3.9560</td> <td>    0.532</td> <td>   -7.439</td> <td> 0.000</td> <td>   -4.998</td> <td>   -2.914</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>teaching</th>  <td>    0.0504</td> <td>    0.009</td> <td>    5.679</td> <td> 0.000</td> <td>    0.033</td> <td>    0.068</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                 Generalized Linear Model Regression Results                  \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   No. Observations:                  467\n",
       "Model:                            GLM   Df Residuals:                      465\n",
       "Model Family:                Binomial   Df Model:                            1\n",
       "Link Function:                  logit   Scale:                          1.0000\n",
       "Method:                          IRLS   Log-Likelihood:                -247.30\n",
       "Date:                Tue, 01 Oct 2019   Deviance:                       494.60\n",
       "Time:                        10:35:52   Pearson chi2:                     457.\n",
       "No. Iterations:                     4   Covariance Type:             nonrobust\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept     -3.9560      0.532     -7.439      0.000      -4.998      -2.914\n",
       "teaching       0.0504      0.009      5.679      0.000       0.033       0.068\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cope teaching y(burnout될 확률)\n",
    "\n",
    "cope 를 통제하니 teaching과 연관성이 - 가 된다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = glm('y ~ teaching + cope', burnout, family=Binomial()).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Generalized Linear Model Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>y</td>        <th>  No. Observations:  </th>  <td>   467</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>GLM</td>       <th>  Df Residuals:      </th>  <td>   464</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model Family:</th>       <td>Binomial</td>     <th>  Df Model:          </th>  <td>     2</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Link Function:</th>        <td>logit</td>      <th>  Scale:             </th> <td>  1.0000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>IRLS</td>       <th>  Log-Likelihood:    </th> <td> -196.55</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>           <td>Tue, 01 Oct 2019</td> <th>  Deviance:          </th> <td>  393.09</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>               <td>10:39:07</td>     <th>  Pearson chi2:      </th>  <td>  563.</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Iterations:</th>         <td>5</td>        <th>  Covariance Type:   </th> <td>nonrobust</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>   -2.0771</td> <td>    0.592</td> <td>   -3.508</td> <td> 0.000</td> <td>   -3.238</td> <td>   -0.917</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>teaching</th>  <td>   -0.0323</td> <td>    0.013</td> <td>   -2.400</td> <td> 0.016</td> <td>   -0.059</td> <td>   -0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cope</th>      <td>    0.1050</td> <td>    0.012</td> <td>    8.443</td> <td> 0.000</td> <td>    0.081</td> <td>    0.129</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                 Generalized Linear Model Regression Results                  \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   No. Observations:                  467\n",
       "Model:                            GLM   Df Residuals:                      464\n",
       "Model Family:                Binomial   Df Model:                            2\n",
       "Link Function:                  logit   Scale:                          1.0000\n",
       "Method:                          IRLS   Log-Likelihood:                -196.55\n",
       "Date:                Tue, 01 Oct 2019   Deviance:                       393.09\n",
       "Time:                        10:39:07   Pearson chi2:                     563.\n",
       "No. Iterations:                     5   Covariance Type:             nonrobust\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept     -2.0771      0.592     -3.508      0.000      -3.238      -0.917\n",
       "teaching      -0.0323      0.013     -2.400      0.016      -0.059      -0.006\n",
       "cope           0.1050      0.012      8.443      0.000       0.081       0.129\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "399.09191057022167"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.aic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2458.8048649881466"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.bic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AIC BIC 는 0 에 가까우면 좋다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예측\n",
    "\n",
    "    (Regression).predict(컬럼명)\n",
    "컬럼값에 대한 예측치가 확률로 나온다 → 실제값과 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.102344\n",
       "1      0.081956\n",
       "2      0.063010\n",
       "3      0.056429\n",
       "4      0.185161\n",
       "5      0.164339\n",
       "6      0.301058\n",
       "7      0.063945\n",
       "8      0.078367\n",
       "9      0.079201\n",
       "10     0.064636\n",
       "11     0.136437\n",
       "12     0.390698\n",
       "13     0.098977\n",
       "14     0.103012\n",
       "15     0.067910\n",
       "16     0.066208\n",
       "17     0.070302\n",
       "18     0.109943\n",
       "19     0.056735\n",
       "20     0.080355\n",
       "21     0.040977\n",
       "22     0.059958\n",
       "23     0.130974\n",
       "24     0.284777\n",
       "25     0.086934\n",
       "26     0.090752\n",
       "27     0.431890\n",
       "28     0.073161\n",
       "29     0.177090\n",
       "         ...   \n",
       "437    0.710186\n",
       "438    0.919098\n",
       "439    0.506222\n",
       "440    0.727535\n",
       "441    0.502291\n",
       "442    0.577847\n",
       "443    0.585499\n",
       "444    0.513022\n",
       "445    0.276714\n",
       "446    0.576446\n",
       "447    0.753375\n",
       "448    0.489064\n",
       "449    0.793407\n",
       "450    0.732601\n",
       "451    0.559605\n",
       "452    0.865039\n",
       "453    0.931986\n",
       "454    0.742028\n",
       "455    0.683301\n",
       "456    0.909339\n",
       "457    0.990306\n",
       "458    0.823070\n",
       "459    0.985958\n",
       "460    0.949901\n",
       "461    0.817589\n",
       "462    0.819295\n",
       "463    0.845213\n",
       "464    0.749365\n",
       "465    0.824736\n",
       "466    0.969114\n",
       "Length: 467, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob = res.predict(burnout)\n",
    "\n",
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,\n",
       "       0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = np.where(prob > .5, 1, 0) # 0.5 가 Threshold\n",
    "\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix 혼돈행렬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- | 예측 0 | 예측 1\n",
    "-|-|-\n",
    "실제 0| - | -\n",
    "실제 1 | - | -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[320,  28],\n",
       "       [ 65,  54]], dtype=int64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true = burnout['y']\n",
    "confusion_matrix(true, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-      | 예측 Not Burnt Out  | 예측 Burnt Out\n",
    "-|-|-\n",
    "실제 Not Burnt Out |  True Negative        320    |   False Positive      28  \n",
    "실제 Burnt Out    |  False Negative        65     |   True Positive      54"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy, Precision, Recall\n",
    "#### Accuracy 정확도\n",
    "$Accuracy = (TN + TP) / (TN + TP + FP + FN) = 378/467$\n",
    "\n",
    "#### Precision 정밀도: 예측 중에\n",
    "$Precision = TP / (TP + FP) = 54/82$\n",
    "\n",
    "    ex. 뽑은 사람 중에 일 잘하는 사람이 얼마\n",
    "    ex. MD 가 잘 팔릴 것 같은 상품을 셀렉했는데 그 중에 잘 팔리는 상품이 얼마나\n",
    "\n",
    "#### Recall 재현도: (True Positive Rate) 실제 중에\n",
    "$Recall = TP / (TP + FN) = 54/119$\n",
    "\n",
    "    ex. 세상에 있는 일 잘하는 사람 중에 얼마나 뽑았는지\n",
    "    ex. 아프리카 돼지 열병 → 다 죽여버리면 Recall 100%\n",
    "    ex. 성범죄자 중에 잡힌 놈이 얼마나 되는지 → 다 잡아넣으면 Recall 100%\n",
    "    \n",
    "> Recall 값만 볼 수 없다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision, Recall 은 0/1 일 때만 쓸 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8008565310492506"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(true, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6585365853658537"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(true, pred, pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.453781512605042"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(true, pred, pos_label=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TPR (True Positive Rate) 과 FTP (False Positive Rate) 의 관계\n",
    "\n",
    "- 모든 주식을 다 산 경우\n",
    "- 오른 주식 중에 내가 산 주식 비율 TPR 100% \n",
    "- 내린 주식 중에 내가 산 주식 비율 FPR 100%\n",
    "- TPR ↑ FPR ↑\n",
    "\n",
    "→ TPR 만 올리고 싶다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curve\n",
    "- 가로축 FPR, 세로축 TPR\n",
    "- 양성(P) 예측을 많이 하면 TPR 과 FPR 이 함께 오른다\n",
    "- 틀린 예측 FPR 에 비해 옳은 예측 TPR 이 얼마나 빨리 오르는가\n",
    "- ROC 곡선이 왼쪽 위로 많이 휠수록(r-shaped) 좋은 분류 방법\n",
    "\n",
    "### Threshold 기준선, 문턱\n",
    "- positive 와 negative 의 기준 경계를 정한다\n",
    "- 예측값(확률)을 어떻게 처리할지 결정한다\n",
    "- 예측값이 몇 % 부터 [불량]이라고 처리할지\n",
    "- 통상적으로 50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, threshold = roc_curve(true, prob, pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FPR</th>\n",
       "      <th>TPR</th>\n",
       "      <th>Threshold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.994476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.002874</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.994476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.002874</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.919098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.005747</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.915520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005747</td>\n",
       "      <td>0.067227</td>\n",
       "      <td>0.909339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.011494</td>\n",
       "      <td>0.067227</td>\n",
       "      <td>0.877704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.011494</td>\n",
       "      <td>0.075630</td>\n",
       "      <td>0.873020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.014368</td>\n",
       "      <td>0.075630</td>\n",
       "      <td>0.871910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.014368</td>\n",
       "      <td>0.092437</td>\n",
       "      <td>0.845213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.020115</td>\n",
       "      <td>0.092437</td>\n",
       "      <td>0.836599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.020115</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.823070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.022989</td>\n",
       "      <td>0.126050</td>\n",
       "      <td>0.819295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.022989</td>\n",
       "      <td>0.159664</td>\n",
       "      <td>0.784345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.028736</td>\n",
       "      <td>0.159664</td>\n",
       "      <td>0.754162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.028736</td>\n",
       "      <td>0.168067</td>\n",
       "      <td>0.753375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.031609</td>\n",
       "      <td>0.168067</td>\n",
       "      <td>0.751235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.031609</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.749365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.031609</td>\n",
       "      <td>0.193277</td>\n",
       "      <td>0.742028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.193277</td>\n",
       "      <td>0.740113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.201681</td>\n",
       "      <td>0.734018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.218487</td>\n",
       "      <td>0.732601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.226891</td>\n",
       "      <td>0.727535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.040230</td>\n",
       "      <td>0.226891</td>\n",
       "      <td>0.711366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.040230</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.710186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.045977</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.707249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.045977</td>\n",
       "      <td>0.243697</td>\n",
       "      <td>0.704866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.048851</td>\n",
       "      <td>0.243697</td>\n",
       "      <td>0.697071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.048851</td>\n",
       "      <td>0.260504</td>\n",
       "      <td>0.673938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.051724</td>\n",
       "      <td>0.268908</td>\n",
       "      <td>0.663153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.051724</td>\n",
       "      <td>0.277311</td>\n",
       "      <td>0.651232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>0.775862</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.073161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>0.778736</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.072102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>0.787356</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.071718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>0.793103</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.070679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>0.798851</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.070302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>0.810345</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.068274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>0.818966</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.067910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>0.821839</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.066564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>0.827586</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.066301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.065946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>0.839080</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.065593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>0.841954</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.064636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>0.847701</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.064290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>0.850575</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.063945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>0.856322</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.063350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>0.864943</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.063010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>0.870690</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.061755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>0.879310</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.059397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>0.887931</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.058759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>0.896552</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.056735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>0.902299</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.056429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>0.905172</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.055598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0.910920</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.055297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>0.916667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.054186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>0.922414</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.053386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>0.933908</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.052027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>0.939655</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.051537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>0.948276</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.050774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>0.954023</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.050498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.017687</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>208 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          FPR       TPR  Threshold\n",
       "0    0.000000  0.000000   1.994476\n",
       "1    0.002874  0.000000   0.994476\n",
       "2    0.002874  0.058824   0.919098\n",
       "3    0.005747  0.058824   0.915520\n",
       "4    0.005747  0.067227   0.909339\n",
       "5    0.011494  0.067227   0.877704\n",
       "6    0.011494  0.075630   0.873020\n",
       "7    0.014368  0.075630   0.871910\n",
       "8    0.014368  0.092437   0.845213\n",
       "9    0.020115  0.092437   0.836599\n",
       "10   0.020115  0.117647   0.823070\n",
       "11   0.022989  0.126050   0.819295\n",
       "12   0.022989  0.159664   0.784345\n",
       "13   0.028736  0.159664   0.754162\n",
       "14   0.028736  0.168067   0.753375\n",
       "15   0.031609  0.168067   0.751235\n",
       "16   0.031609  0.176471   0.749365\n",
       "17   0.031609  0.193277   0.742028\n",
       "18   0.034483  0.193277   0.740113\n",
       "19   0.034483  0.201681   0.734018\n",
       "20   0.034483  0.218487   0.732601\n",
       "21   0.034483  0.226891   0.727535\n",
       "22   0.040230  0.226891   0.711366\n",
       "23   0.040230  0.235294   0.710186\n",
       "24   0.045977  0.235294   0.707249\n",
       "25   0.045977  0.243697   0.704866\n",
       "26   0.048851  0.243697   0.697071\n",
       "27   0.048851  0.260504   0.673938\n",
       "28   0.051724  0.268908   0.663153\n",
       "29   0.051724  0.277311   0.651232\n",
       "..        ...       ...        ...\n",
       "178  0.775862  1.000000   0.073161\n",
       "179  0.778736  1.000000   0.072102\n",
       "180  0.787356  1.000000   0.071718\n",
       "181  0.793103  1.000000   0.070679\n",
       "182  0.798851  1.000000   0.070302\n",
       "183  0.810345  1.000000   0.068274\n",
       "184  0.818966  1.000000   0.067910\n",
       "185  0.821839  1.000000   0.066564\n",
       "186  0.827586  1.000000   0.066301\n",
       "187  0.833333  1.000000   0.065946\n",
       "188  0.839080  1.000000   0.065593\n",
       "189  0.841954  1.000000   0.064636\n",
       "190  0.847701  1.000000   0.064290\n",
       "191  0.850575  1.000000   0.063945\n",
       "192  0.856322  1.000000   0.063350\n",
       "193  0.864943  1.000000   0.063010\n",
       "194  0.870690  1.000000   0.061755\n",
       "195  0.879310  1.000000   0.059397\n",
       "196  0.887931  1.000000   0.058759\n",
       "197  0.896552  1.000000   0.056735\n",
       "198  0.902299  1.000000   0.056429\n",
       "199  0.905172  1.000000   0.055598\n",
       "200  0.910920  1.000000   0.055297\n",
       "201  0.916667  1.000000   0.054186\n",
       "202  0.922414  1.000000   0.053386\n",
       "203  0.933908  1.000000   0.052027\n",
       "204  0.939655  1.000000   0.051537\n",
       "205  0.948276  1.000000   0.050774\n",
       "206  0.954023  1.000000   0.050498\n",
       "207  1.000000  1.000000   0.017687\n",
       "\n",
       "[208 rows x 3 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'FPR':fpr, 'TPR':tpr, 'Threshold': threshold})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x21c66f00b38>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.lineplot(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 감수할 수 있는 False Positive Rate 를 정해놓는다\n",
    "    \n",
    "    ex. FPR 멀쩡한 제품을 불량으로 판단해서 폐기하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC (Area Under Curve)\n",
    "- ROC curve 아래 쪽의 면적\n",
    "\n",
    "auc|예측 정도\n",
    "-|-\n",
    "0.5|무작위\n",
    "1|완벽\n",
    "\n",
    "- auc 가 0.5 아래면 안하느니만 못하다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8729957500241475"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실제 데이터 분석에 적용\n",
    "Data Set 을 두 개 준비\n",
    "\n",
    "- train data 에서 ROC Curve, auc 확인해서, Model Parameter 결정 \n",
    "- test data 에 적용, 성능지표 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 선형모형이 아닌 다른 예측 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest Neighbor 최근접 이웃\n",
    "- 간단\n",
    "- 예측하고자 하는 사례와 가장 비슷한 사례를 k 개 찾아 동일하게 예측\n",
    "- 유사도를 정할 수 있어야 한다(수치화)\n",
    "- 데이터가 많아지면 시간이 오래 걸린다\n",
    "- 데이터가 적으면 많은 사례수를 찾을수록 예측력이 떨어진다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree 의사결정 나무\n",
    "- 스무고개처럼 Y/N 질문을 반복\n",
    "- 질문의 수가 적으면 해석하기 쉽다\n",
    "- 데이터가 조금만 달라져도 결과가 크게 달라진다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble 앙상블\n",
    "- 하나의 모형으로 예측을 잘하는데 한계가 있다\n",
    "- 여러 개의 모형을 만들어 평균/다수결로 예측하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "대회(정확도)와 실무(비용, 시간)는 다르다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "- 데이터에서 비슷한 사례끼리 cluster 로 묶어주는 것\n",
    "- Clustering 에는 정답이 없다 \n",
    "> cf. 분류 0/1\n",
    "\n",
    "-|분류 | Clustering\n",
    "-|-|-\n",
    "범주|관찰변수|잠재변수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering 의 종류\n",
    "- 아주 많은데, 아래 3가지 정도만 현업에서 사용\n",
    "- k-means\n",
    "- Hierachical Clustering\n",
    "- DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-means\n",
    "- k 개의 평균 means\n",
    "- 가장 널리 사용되는 Clustering 방법\n",
    "- 군집의 평균(중심점)을 구한다\n",
    "- 사례를 중심점이 가장 가까운 군집에 포함시킨다\n",
    "\n",
    "#### k-means Clustering 방법\n",
    "- 중심점 k 개를 임의로 정한다\n",
    "\n",
    "> Domain Knowledge 가 중요하다. ex. 우리 고객은 두 종류다\n",
    "\n",
    "- 데이터를 가장 가까운 중심점 별로 나눈다\n",
    "- 나누어진 Cluster 별 평균을 구한다\n",
    "- 계산한 평균을 중심점으로 삼아 다시 데이터를 나눈다\n",
    "- Cluster 가 바뀌지 않을 때까지 반복\n",
    "\n",
    "     → 데이터가 너무 많으면, 소수의 사례만 무작위로 뽑아 Clustering 한다(미니배치 k-means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-means Clustering 할 때 중요하지 않은 변수는 제외한다. (Domain Knowledge)\n",
    "> ex. 성별, 사는 지역"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>MalicAcid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>AlcalinityOfAsh</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>TotalPhenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>NonflavanoidPhenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD</th>\n",
       "      <th>Proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>3</td>\n",
       "      <td>12.25</td>\n",
       "      <td>4.72</td>\n",
       "      <td>2.54</td>\n",
       "      <td>21.0</td>\n",
       "      <td>89</td>\n",
       "      <td>1.38</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.8</td>\n",
       "      <td>3.85</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.27</td>\n",
       "      <td>720</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Class  Alcohol  MalicAcid   Ash  AlcalinityOfAsh  Magnesium  \\\n",
       "136      3    12.25       4.72  2.54             21.0         89   \n",
       "\n",
       "     TotalPhenols  Flavanoids  NonflavanoidPhenols  Proanthocyanins  \\\n",
       "136          1.38        0.47                 0.53              0.8   \n",
       "\n",
       "     Color intensity   Hue    OD  Proline  \n",
       "136             3.85  0.75  1.27      720  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine = pd.read_csv('wine.csv')\n",
    "\n",
    "wine.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>MalicAcid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>AlcalinityOfAsh</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>TotalPhenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>NonflavanoidPhenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD</th>\n",
       "      <th>Proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>13.32</td>\n",
       "      <td>3.24</td>\n",
       "      <td>2.38</td>\n",
       "      <td>21.5</td>\n",
       "      <td>92</td>\n",
       "      <td>1.93</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.45</td>\n",
       "      <td>1.25</td>\n",
       "      <td>8.42</td>\n",
       "      <td>0.55</td>\n",
       "      <td>1.62</td>\n",
       "      <td>650</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Alcohol  MalicAcid   Ash  AlcalinityOfAsh  Magnesium  TotalPhenols  \\\n",
       "148    13.32       3.24  2.38             21.5         92          1.93   \n",
       "\n",
       "     Flavanoids  NonflavanoidPhenols  Proanthocyanins  Color intensity   Hue  \\\n",
       "148        0.76                 0.45             1.25             8.42  0.55   \n",
       "\n",
       "       OD  Proline  \n",
       "148  1.62      650  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# .iloc[] 번호로 위치를 지정\n",
    "# : 모든 행, 1: = 1 열부터\n",
    "\n",
    "data = wine.iloc[:, 1:] \n",
    "data.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling\n",
    "- 수치별로 변화의 폭이 다르기 때문에\n",
    "- 크게 변화하는 수치에 크게 좌우된다\n",
    "\n",
    "→ Scaling 해서 변화 폭을 일정하게 만든다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min-Max Scaling\n",
    "단점: Outlier 있을 때 영향을 많이 받는다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Magnesium'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Magnesium'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization 표준화\n",
    "- 모든 값에서 평균을 빼고, 표준편차로 나눠주는 것\n",
    "- 많이 쓴다\n",
    "- 고객을 유형별로 Clustering 할 때 사용\n",
    "> ex. 추천시스템 등"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Minju\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scale = StandardScaler()\n",
    "\n",
    "scale.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Minju\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "x = scale.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=3) # 3 개의 Cluster 로 묶어줘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=3, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km.fit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3], dtype=int64)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine['Class'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wine['Class'] 를 보고 Clustering 한 것은 아니지만, 비슷한 것끼리 묶었더니 결과적으로 비슷해진다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affinity Propagation\n",
    "- 잘 쓰지는 않는다\n",
    "- 중심점 대신 대표(examplar)를 찾는다\n",
    "> ex. 새의 exemplar 는 참새\n",
    "    \n",
    "#### Affinity Propagation Clustering 방법\n",
    "- responsibility 와 availability 라는 수치를 반복적으로 계산한다\n",
    "> cf. k-means 에서는 평균(중심값)을 반복적으로 계산\n",
    "- 각 사례의 대표가 바뀌지 않을 때까지 반복한다\n",
    "- Cluster 의 개수를 미리 정하지 않는다\n",
    "- 데이터가 적을 때 사용한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AffinityPropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AffinityPropagation(affinity='euclidean', convergence_iter=15, copy=True,\n",
       "          damping=0.5, max_iter=200, preference=-200, verbose=False)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preference 얼마나 다르면 나눌지 결정하는 factor\n",
    "# 숫자가 클수록 더 많이 나눈다\n",
    "# 정하기 애매하다 \n",
    "  \n",
    "ap = AffinityPropagation(preference=-200)\n",
    "\n",
    "ap.fit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 1, 1, 1, 1,\n",
       "       1, 1, 2, 1, 2, 1, 1, 0, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2], dtype=int64)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ap.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Shift\n",
    "- Cluster 의 개수를 미리 정하지 않는다\n",
    "- bandwidth(반지름) 정한다\n",
    "- 잘 안 쓴다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MeanShift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MeanShift(bandwidth=3, bin_seeding=False, cluster_all=True, min_bin_freq=1,\n",
       "     n_jobs=None, seeds=None)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms = MeanShift(bandwidth=3)\n",
    "\n",
    "ms.fit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 1, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 2, 1, 0, 0, 5, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 2, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       9, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 8, 4, 4, 4, 0, 0, 0, 0, 4, 7, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1], dtype=int64)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral Clustering\n",
    "- 데이터 안에서 가까운 점끼리 잇는다(graph)\n",
    "- 잘 안 쓴다\n",
    "> ex. 페이스북 친구여부로 Spectral Clustering 할 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0,\n",
       "       0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = SpectralClustering(n_clusters=3)\n",
    "\n",
    "sc.fit_predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Clustering\n",
    "- 비슷한 Cluster 끼리 합치는 것을 반복한다\n",
    "> ex. 친구의 친구의 친구의 친구도 다 친구\n",
    "- Cluster 의 수를 정해주어야 하지만, 나중에 정할 수 있다\n",
    "- 데이터가 많아도 작동한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 1, 1, 0, 0, 0, 2,\n",
       "       2, 0, 1, 0, 1, 2, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1], dtype=int64)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ac = AgglomerativeClustering(n_clusters=3)\n",
    "ac.fit_predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN\n",
    "- Mean Shift 와 비슷\n",
    "- 친구의 친구 찾기\n",
    "- Cluster 의 수를 미리 정하지 않아도 된다\n",
    "- 단점: 혼자 멀리 떨어져있으면 Clustering 에 포함이 안 된다\n",
    "\n",
    "#### DBSCAN Clustering 방법\n",
    "- 일정거리$ε$ 에 최소 $N$ 개의 다른 사례가 있으면 핵심점 Core Point 로 정한다\n",
    "- 거리가 $ε$ 보다 가까운 핵심점끼리 연결한다\n",
    "- 연결된 핵심점들과 그 핵심점에서 $ε$ 만큼 떨어진 점들로 Cluster 를 만든다\n",
    "\n",
    "Mean Shift | DBSCAN\n",
    "-|-\n",
    "평균을 사용하기 때문에 k-means 와 결과가 비슷하다| 친구의 친구를 따라가는 방법이라 데이터가 뭉쳐있지 않아도 쓸 수 있다\n",
    "데이터가 적을 때만 |Spectral Clustering 등에 비해 대용량 데이터에서 잘 작동한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0, -1,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0, -1,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1, -1,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0, -1,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0, -1,  0,  0, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0, -1, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0], dtype=int64)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# min_sample 주변에 인싸가 5명은 있어야 Cluster 안에 들어갈 수 있다\n",
    "ds = DBSCAN(eps=3, min_samples=5) \n",
    "\n",
    "ds.fit_predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "→ -1 은 혼자 따로 떨어져 있는 값"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPTICS\n",
    "- DBSCAN 과 비슷하지만 일정거리 $ε$ 를 안 정해줘도 되는 방법\n",
    "- Reachability 값을 계산해 그 값에 따라 Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BIRCH\n",
    "- 대용량 데이터를 빠르게 Clustering 하기 위한 알고리즘\n",
    "- 실제로 쓰는 경우는 잘 못 봤다\n",
    "- 변수가 많을 때는 비효율적이다\n",
    "\n",
    "#### BIRCH Clustering 방법\n",
    "- 트리 구조로 subcluster 를 먼저 만들고\n",
    "- 마지막에 subcluster 를 합쳐서 Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering 평가\n",
    "- 사례들이 Cluster 의 중심에서 얼마나 가까운가\n",
    "> 낮을수록 좋다. 0이 가장 좋다\n",
    "- 다른 Cluster 와 얼마나 다른가\n",
    "> 높을수록 좋다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering 평가 지표의 한계\n",
    "- k-means 와 같은 경우에만 유효\n",
    "- Clustering 결과를 바탕으로 행동했을 때 그 성과로 판단한다\n",
    "- Regression 과 달리 평가지표를 열심히 볼 필요 없다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
